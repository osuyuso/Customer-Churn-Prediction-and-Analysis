{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Churn Prediction & Analysis\n",
        "\n",
        "End-to-end notebook covering all project milestones: data intake, EDA, advanced analysis, modeling, MLOps handoff, and deployment-ready artifacts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Milestone Roadmap\n",
        "1. **Data Collection & EDA:** Load Kaggle churn dataset, inspect structure, clean anomalies, visualize.\n",
        "2. **Advanced Analysis & Feature Engineering:** Statistical tests, RFE-style importance, engineered interaction features.\n",
        "3. **Modeling & Optimization:** Train/evaluate multiple classifiers with cross-validation + hyperparameter tuning.\n",
        "4. **MLOps & Deployment:** Persist artifacts, log metrics, expose FastAPI prediction example, outline monitoring.\n",
        "5. **Documentation & Presentation Ready Assets:** Auto-generate cleaned datasets, metrics tables, and summary report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Modeling / preprocessing\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    RocCurveDisplay,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Persistence / logging\n",
        "import joblib\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = Path('../customer_churn_dataset-testing-master.csv').resolve()\n",
        "ARTIFACT_DIR = Path('../artifacts')\n",
        "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Dataset located at: {DATA_PATH}\")\n",
        "print(f\"Artifacts will be saved to: {ARTIFACT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_df = pd.read_csv(DATA_PATH)\n",
        "print(f\"Shape: {raw_df.shape}\")\n",
        "raw_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dataset_overview(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    print(\"\\n--- Schema ---\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\n--- Missing Values ---\")\n",
        "    print(df.isna().sum())\n",
        "    print(\"\\n--- Duplicates ---\")\n",
        "    print(df.duplicated().sum())\n",
        "    return df.describe(include='all').T\n",
        "\n",
        "overview = dataset_overview(raw_df)\n",
        "overview.head(12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "sns.countplot(x='Churn', data=raw_df, ax=ax[0], palette='coolwarm')\n",
        "ax[0].set_title('Churn Distribution')\n",
        "raw_df['Churn'].value_counts(normalize=True).plot(kind='pie', autopct='%1.1f%%', ax=ax[1], colors=['#0B5563', '#F26419'])\n",
        "ax[1].set_ylabel('')\n",
        "ax[1].set_title('Churn Share')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "num_cols = [c for c in raw_df.select_dtypes(include=np.number).columns if c != 'Churn']\n",
        "raw_df[num_cols].hist(bins=30, figsize=(14, 10))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "for col in ['Gender', 'Subscription Type', 'Contract Length']:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.barplot(x=col, y='Churn', data=raw_df, estimator=np.mean)\n",
        "    plt.title(f'Average Churn by {col}')\n",
        "    plt.xticks(rotation=30)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_for_corr = raw_df.copy()\n",
        "cat_cols = encoded_for_corr.select_dtypes(include='object').columns\n",
        "encoded_for_corr = pd.get_dummies(encoded_for_corr, columns=cat_cols, drop_first=True)\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(encoded_for_corr.corr(), cmap='coolwarm', center=0)\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    cleaned = df.copy()\n",
        "    cleaned = cleaned.drop_duplicates().reset_index(drop=True)\n",
        "    cleaned = cleaned.dropna()\n",
        "    return cleaned\n",
        "\n",
        "clean_df = clean_data(raw_df)\n",
        "print(f\"Rows removed: {raw_df.shape[0] - clean_df.shape[0]}\")\n",
        "clean_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_path = ARTIFACT_DIR / 'cleaned_customer_churn.csv'\n",
        "clean_df.to_csv(clean_path, index=False)\n",
        "print(f\"Saved cleaned dataset to {clean_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Milestone 2: Statistical Tests & Feature Signals ---\n",
        "cat_columns = ['Gender', 'Subscription Type', 'Contract Length']\n",
        "stat_results = []\n",
        "for col in cat_columns:\n",
        "    contingency = pd.crosstab(clean_df[col], clean_df['Churn'])\n",
        "    chi2, p, _, _ = stats.chi2_contingency(contingency)\n",
        "    stat_results.append({'feature': col, 'test': 'chi2', 'p_value': p, 'chi2': chi2})\n",
        "\n",
        "num_columns = ['Age', 'Tenure', 'Usage Frequency', 'Support Calls', 'Payment Delay', 'Total Spend', 'Last Interaction']\n",
        "for col in num_columns:\n",
        "    churned = clean_df[clean_df['Churn'] == 1][col]\n",
        "    stayed = clean_df[clean_df['Churn'] == 0][col]\n",
        "    stat, p = stats.ttest_ind(churned, stayed, equal_var=False)\n",
        "    stat_results.append({'feature': col, 'test': 't-test', 'p_value': p, 'statistic': stat})\n",
        "\n",
        "stat_df = pd.DataFrame(stat_results)\n",
        "stat_df.sort_values('p_value').head(12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_df = clean_df.copy()\n",
        "feature_df['TenureBucket'] = pd.cut(\n",
        "    feature_df['Tenure'],\n",
        "    bins=[0, 12, 24, 48, 60, 100],\n",
        "    labels=['<1y', '1-2y', '2-4y', '4-5y', '5y+']\n",
        ")\n",
        "feature_df['HighSupport'] = (feature_df['Support Calls'] > feature_df['Support Calls'].median()).astype(int)\n",
        "feature_df['RecentInteraction'] = (feature_df['Last Interaction'] <= feature_df['Last Interaction'].median()).astype(int)\n",
        "feature_df['UsagePerTenure'] = feature_df['Usage Frequency'] / (feature_df['Tenure'].replace(0, np.nan))\n",
        "feature_df['UsagePerTenure'] = feature_df['UsagePerTenure'].fillna(feature_df['UsagePerTenure'].median())\n",
        "feature_df['SpendPerMonth'] = feature_df['Total Spend'] / feature_df['Contract Length'].map({'Monthly':1, 'Quarterly':3, 'Annual':12}).replace({0: np.nan})\n",
        "feature_df['SpendPerMonth'] = feature_df['SpendPerMonth'].fillna(feature_df['SpendPerMonth'].median())\n",
        "feature_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_df = feature_df.drop(columns=['CustomerID'])\n",
        "target = feature_df['Churn']\n",
        "X = feature_df.drop(columns=['Churn'])\n",
        "y = target\n",
        "\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "print(f\"Churn rate train: {y_train.mean():.2%} | test: {y_test.mean():.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_reg = Pipeline(steps=[\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "rfecv = RFECV(\n",
        "    estimator=log_reg['clf'],\n",
        "    step=1,\n",
        "    cv=StratifiedKFold(5),\n",
        "    scoring='f1',\n",
        "    min_features_to_select=5\n",
        ")\n",
        "\n",
        "X_prepared = preprocessor.fit_transform(X_train)\n",
        "rfecv.fit(X_prepared, y_train)\n",
        "\n",
        "selected_mask = rfecv.support_\n",
        "feature_names = np.concatenate([\n",
        "    numeric_features,\n",
        "    preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "])\n",
        "selected_features = feature_names[selected_mask]\n",
        "print(f\"Selected top {len(selected_features)} features via RFECV\")\n",
        "pd.Series(rfecv.grid_scores_, index=range(len(rfecv.grid_scores_))).plot(title='RFECV F1 by feature count')\n",
        "plt.xlabel('Number of features')\n",
        "plt.ylabel('Mean CV F1')\n",
        "plt.show()\n",
        "pd.Series(selected_features).head(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(model_name: str, estimator: Pipeline) -> dict:\n",
        "    estimator.fit(X_train, y_train)\n",
        "    preds = estimator.predict(X_test)\n",
        "    probs = estimator.predict_proba(X_test)[:, 1]\n",
        "    report = classification_report(y_test, preds, output_dict=True)\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    roc = roc_auc_score(y_test, probs)\n",
        "    print(f\"\\n===== {model_name} =====\")\n",
        "    print(classification_report(y_test, preds))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "    RocCurveDisplay.from_predictions(y_test, probs)\n",
        "    plt.title(f'{model_name} ROC Curve')\n",
        "    plt.show()\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'precision': report['1']['precision'],\n",
        "        'recall': report['1']['recall'],\n",
        "        'f1': report['1']['f1-score'],\n",
        "        'roc_auc': roc,\n",
        "        'estimator': estimator\n",
        "    }\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': Pipeline([\n",
        "        ('prep', preprocessor),\n",
        "        ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "    ]),\n",
        "    'Random Forest': Pipeline([\n",
        "        ('prep', preprocessor),\n",
        "        ('clf', RandomForestClassifier(n_estimators=300, max_depth=None, class_weight='balanced'))\n",
        "    ]),\n",
        "    'Gradient Boosting': Pipeline([\n",
        "        ('prep', preprocessor),\n",
        "        ('clf', GradientBoostingClassifier())\n",
        "    ])\n",
        "}\n",
        "\n",
        "results = [train_and_evaluate(name, pipe) for name, pipe in models.items()]\n",
        "results_df = pd.DataFrame([{k: v for k, v in res.items() if k != 'estimator'} for res in results])\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_model_name = results_df.sort_values('f1', ascending=False).iloc[0]['model']\n",
        "print(f\"Top baseline model: {top_model_name}\")\n",
        "\n",
        "search_spaces = {\n",
        "    'Random Forest': {\n",
        "        'clf__n_estimators': [200, 400, 600],\n",
        "        'clf__max_depth': [None, 10, 20, 30],\n",
        "        'clf__min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'clf__learning_rate': [0.05, 0.1, 0.2],\n",
        "        'clf__n_estimators': [200, 400],\n",
        "        'clf__max_depth': [3, 4]\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'clf__C': [0.1, 1, 10],\n",
        "        'clf__penalty': ['l2'],\n",
        "        'clf__solver': ['lbfgs']\n",
        "    }\n",
        "}\n",
        "\n",
        "best_estimators = {}\n",
        "for model_name, pipe in models.items():\n",
        "    param_grid = search_spaces[model_name]\n",
        "    grid = GridSearchCV(pipe, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_estimators[model_name] = grid.best_estimator_\n",
        "    print(f\"{model_name} best params: {grid.best_params_} | best F1: {grid.best_score_:.3f}\")\n",
        "\n",
        "# Evaluate tuned model with best test F1\n",
        "best_model_name = None\n",
        "best_f1 = -np.inf\n",
        "best_model = None\n",
        "for model_name, estimator in best_estimators.items():\n",
        "    preds = estimator.predict(X_test)\n",
        "    probs = estimator.predict_proba(X_test)[:, 1]\n",
        "    f1 = precision_recall_fscore_support(y_test, preds, average='binary')[2]\n",
        "    if f1 > best_f1:\n",
        "        best_model_name = model_name\n",
        "        best_f1 = f1\n",
        "        best_model = estimator\n",
        "\n",
        "print(f\"Selected best model: {best_model_name} | Test F1: {best_f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = ARTIFACT_DIR / f\"best_model_{best_model_name.replace(' ', '_').lower()}.joblib\"\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f\"Persisted best model to {model_path}\")\n",
        "\n",
        "# Save metadata for deployment\n",
        "metadata = {\n",
        "    'model_name': best_model_name,\n",
        "    'test_f1': float(best_f1),\n",
        "    'test_precision': float(precision_recall_fscore_support(y_test, best_model.predict(X_test), average='binary')[0]),\n",
        "    'test_recall': float(precision_recall_fscore_support(y_test, best_model.predict(X_test), average='binary')[1]),\n",
        "    'roc_auc': float(roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1])),\n",
        "    'generated_at': datetime.utcnow().isoformat()\n",
        "}\n",
        "meta_path = ARTIFACT_DIR / 'model_report.json'\n",
        "with open(meta_path, 'w') as fp:\n",
        "    json.dump(metadata, fp, indent=2)\n",
        "print(f\"Saved metadata to {meta_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Milestone 4: Deployment Blueprint\n",
        "The snippet below shows a minimal FastAPI service that:\n",
        "- loads the persisted `joblib` pipeline\n",
        "- exposes `/predict` for real-time churn scoring\n",
        "- performs simple logging for monitoring hooks\n",
        "\n",
        "> Run it via `uvicorn api.app:app --reload` once the model artifact exists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fastapi_example = \"\"\"\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('artifacts/model_report.json', 'r') as fh:\n",
        "    meta = json.load(fh)\n",
        "model_path = meta['model_path']\n",
        "model = joblib.load(model_path)\n",
        "app = FastAPI(title='Churn Predictor')\n",
        "\n",
        "class Customer(BaseModel):\n",
        "    Age: int\n",
        "    Gender: str\n",
        "    Tenure: int\n",
        "    Usage_Frequency: int\n",
        "    Support_Calls: int\n",
        "    Payment_Delay: int\n",
        "    Subscription_Type: str\n",
        "    Contract_Length: str\n",
        "    Total_Spend: float\n",
        "    Last_Interaction: int\n",
        "    TenureBucket: str\n",
        "    HighSupport: int\n",
        "    RecentInteraction: int\n",
        "    UsagePerTenure: float\n",
        "    SpendPerMonth: float\n",
        "\n",
        "@app.post('/predict')\n",
        "def predict(customer: Customer):\n",
        "    df = pd.DataFrame([customer.dict()])\n",
        "    prob = model.predict_proba(df)[0][1]\n",
        "    return {'churn_probability': float(prob), 'churn_risk': prob > 0.5}\n",
        "\"\"\"\n",
        "print(fastapi_example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitoring & Retraining Checklist\n",
        "- **Data Drift:** schedule weekly Kolmogorovâ€“Smirnov tests comparing live features vs training distribution.\n",
        "- **Performance Alerts:** recompute precision/recall from labeled feedback; alert if F1 drops >5% absolute.\n",
        "- **Logging:** persist every API call payload + prediction + eventual true label for audit trail.\n",
        "- **Retraining Trigger:** gather at least 5k new labeled observations or once drift/performance thresholds breach.\n",
        "- **Versioning:** track models + metrics via MLflow/DVC, store experiment IDs in `model_report.json`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Milestone 5: Executive-Ready Summary\n",
        "- Clean dataset saved to `artifacts/cleaned_customer_churn.csv`.\n",
        "- Feature-engineered dataset resides in notebook memory; export if needed for AutoML.\n",
        "- Model leaderboard stored in `results_df`; selected best model + metrics saved to `artifacts/model_report.json`.\n",
        "- Deployment playbook + FastAPI snippet included for quick operationalization.\n",
        "- Use the companion `reports/final_project_report.md` for stakeholder narrative + slide-ready bullets.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
